{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olgasherbiena/KPI/blob/main/%D0%90%D0%9A_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABND7PIbPhQW",
        "outputId": "f2f52c6f-a075-494c-9307-d20b8dc088f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import kagglehub\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "csv_path  = kagglehub.dataset_download(\"shantanudhakadd/email-spam-detection-dataset-classification\")\n",
        "csv_file = os.path.join(csv_path, 'spam.csv')\n",
        "df = pd.read_csv(csv_file, encoding='latin1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m74dbY160h4m",
        "outputId": "5308713f-0f8b-43d5-8f9e-833ce215f468"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/shantanudhakadd/email-spam-detection-dataset-classification?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 211k/211k [00:00<00:00, 6.74MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÐŸÑ–Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð°Ð½Ð¸Ñ…"
      ],
      "metadata": {
        "id": "GiiQuiNDhu72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['v1', 'v2']] # v1: 'label', v2: 'text'\n",
        "df.columns = ['label', 'message']\n",
        "df = df.iloc[:,:2]\n",
        "df.columns = ['label', 'message']\n",
        "le = LabelEncoder()\n",
        "df['label_encoded'] = le.fit_transform(df['label'])\n",
        "\n",
        "# Ð’ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ñ–Ñ Ñ‚ÐµÐºÑÑ‚Ð¾Ð²Ð¸Ñ… Ð´Ð°Ð½Ð¸Ñ… Ð·Ð° Ð´Ð¾Ð¿Ð¾Ð¼Ð¾Ð³Ð¾ÑŽ TF-IDF\n",
        "# TF-IDF Ð¿ÐµÑ€ÐµÑ‚Ð²Ð¾Ñ€ÑŽÑ” Ñ‚ÐµÐºÑÑ‚ Ð½Ð° Ñ‡Ð¸ÑÐ»Ð¾Ð²Ñ– Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¸, Ð½ÐµÐ¾Ð±Ñ…Ñ–Ð´Ð½Ñ– Ð´Ð»Ñ LSH Ñ‚Ð° Logistic Regression\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=500) # ÐžÐ±Ð¼ÐµÐ¶ÑƒÑ”Ð¼Ð¾ Ñ€Ð¾Ð·Ð¼Ñ–Ñ€Ð½Ñ–ÑÑ‚ÑŒ Ð´Ð»Ñ Ð¿Ñ€Ð¸ÐºÐ»Ð°Ð´Ñƒ\n",
        "X_tfidf = tfidf.fit_transform(df['message']).toarray()\n",
        "y = df['label_encoded'].values"
      ],
      "metadata": {
        "id": "tBJBOI7Igw-5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ð ÐµÐ°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ"
      ],
      "metadata": {
        "id": "YOzGlZBPhzyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# ÐŸÑ€Ð¸Ð¿ÑƒÑÑ‚Ð¸Ð¼Ð¾, Ñ‰Ð¾ X_tfidf Ð²Ð¶Ðµ Ð²Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¾ Ñ– Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾\n",
        "\n",
        "# --- 1. ÐŸÐžÐ’ÐÐ• Ð’Ð˜Ð—ÐÐÐ§Ð•ÐÐÐ¯ ÐšÐ›ÐÐ¡Ð£ LSH (Ð’ÐšÐ›Ð®Ð§ÐÐ®Ð§Ð˜ __init__) ---\n",
        "class LSH:\n",
        "    def __init__(self, num_buckets, num_hash_functions, input_dim):\n",
        "        # ÐšÐ¾Ð½ÑÑ‚Ñ€ÑƒÐºÑ‚Ð¾Ñ€, ÑÐºÐ¸Ð¹ Ð¿Ñ€Ð¸Ð¹Ð¼Ð°Ñ” 3 Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð¸\n",
        "        self.num_buckets = num_buckets\n",
        "        self.num_hash_functions = num_hash_functions\n",
        "        self.input_dim = input_dim\n",
        "        self.hash_functions = [self._generate_hash_function() for _ in range(num_hash_functions)]\n",
        "        self.buckets = {i: [] for i in range(num_buckets)}\n",
        "\n",
        "    def _generate_hash_function(self):\n",
        "        return np.random.randn(self.input_dim)\n",
        "\n",
        "    def hash(self, vector):\n",
        "        hashes = []\n",
        "        for hash_function in self.hash_functions:\n",
        "            hash_value = np.dot(vector, hash_function)\n",
        "            hash_value = hash_value / np.linalg.norm(hash_function)\n",
        "            # ÐŸÑ€Ð¸Ð¼Ñ–Ñ‚ÐºÐ°: Ð´Ð»Ñ LSH (Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ñ– SimHash) Ñ‡Ð°ÑÑ‚Ð¾ Ð²Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑŽÑ‚ÑŒ np.sign,\n",
        "            # Ð°Ð»Ðµ Ð¼Ð¸ Ð·Ð°Ð»Ð¸ÑˆÐ°Ñ”Ð¼Ð¾ Ð²Ð°ÑˆÑƒ Ð»Ð¾Ð³Ñ–ÐºÑƒ Ð· np.astype(int) Ð´Ð»Ñ Ñ†Ñ–Ð»Ð¸Ñ… Ñ…ÐµÑˆÑ–Ð².\n",
        "            hash_value = hash_value.astype(int)\n",
        "            hashes.append(hash_value)\n",
        "        return hashes\n",
        "\n",
        "    def add_vector(self, vector, data_index): # Ð”Ð¾Ð´Ð°Ð½Ð¾ data_index Ð´Ð»Ñ ÐºÐ¾Ñ€ÐµÐºÑ‚Ð½Ð¾Ñ— Ð¾Ð¿Ñ‚Ð¸Ð¼Ñ–Ð·Ð°Ñ†Ñ–Ñ—\n",
        "        hashes = self.hash(vector)\n",
        "        for hash_value in hashes:\n",
        "            # Ð—Ð±ÐµÑ€Ñ–Ð³Ð°Ñ”Ð¼Ð¾ (Ð¡ÐŸÐ ÐÐ’Ð–ÐÐ†Ð™ Ð†ÐÐ”Ð•ÐšÐ¡, Ð’Ð•ÐšÐ¢ÐžÐ )\n",
        "            self.buckets[hash_value % self.num_buckets].append((data_index, vector))\n",
        "\n",
        "    def query(self, query_vector):\n",
        "        query_hashes = self.hash(query_vector)\n",
        "\n",
        "        # Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð¾Ð²ÑƒÑ”Ð¼Ð¾ set Ð´Ð»Ñ Ð¨Ð’Ð˜Ð”ÐšÐžÐ“Ðž Ð²Ñ–Ð´ÑÑ‚ÐµÐ¶ÐµÐ½Ð½Ñ ÑƒÐ½Ñ–ÐºÐ°Ð»ÑŒÐ½Ð¸Ñ… Ñ–Ð½Ð´ÐµÐºÑÑ–Ð² (ÐžÐ¿Ñ‚Ð¸Ð¼Ñ–Ð·Ð°Ñ†Ñ–Ñ)\n",
        "        similar_indices = set()\n",
        "\n",
        "        for hash_value in query_hashes:\n",
        "            bucket = self.buckets[hash_value % self.num_buckets]\n",
        "\n",
        "            for index, vector in bucket:\n",
        "\n",
        "                if np.array_equal(vector, query_vector):\n",
        "                    continue\n",
        "\n",
        "                # Ð¨Ð²Ð¸Ð´ÐºÐµ Ð´Ð¾Ð´Ð°Ð²Ð°Ð½Ð½Ñ Ñ–Ð½Ð´ÐµÐºÑÑƒ Ð´Ð¾ set Ð´Ð»Ñ ÑƒÐ½Ñ–ÐºÐ°Ð»ÑŒÐ½Ð¾ÑÑ‚Ñ–\n",
        "                similar_indices.add(index)\n",
        "\n",
        "        # ÐŸÐ¾Ð²ÐµÑ€Ñ‚Ð°Ñ”Ð¼Ð¾ ÑÐ¿Ð¸ÑÐ¾Ðº Ð£ÐÐ†ÐšÐÐ›Ð¬ÐÐ˜Ð¥ Ð†ÐÐ”Ð•ÐšÐ¡Ð†Ð’\n",
        "        return list(similar_indices)\n",
        "\n",
        "# --- 2. Ð†ÐÐ†Ð¦Ð†ÐÐ›Ð†Ð—ÐÐ¦Ð†Ð¯ Ð¢Ð Ð’Ð˜ÐšÐžÐÐÐÐÐ¯ ---\n",
        "\n",
        "input_dim = X_tfidf.shape[1]\n",
        "# ÐžÐ¿Ñ‚Ð¸Ð¼Ñ–Ð·Ð°Ñ†Ñ–Ñ: Ð·Ð¼ÐµÐ½ÑˆÑƒÑ”Ð¼Ð¾ ÐºÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ Ñ…ÐµÑˆ-Ñ„ÑƒÐ½ÐºÑ†Ñ–Ð¹ Ð´Ð¾ 15\n",
        "lsh_model = LSH(num_buckets=100, num_hash_functions=15, input_dim=input_dim)\n",
        "\n",
        "# Ð—Ð°Ð¿Ð¾Ð²Ð½ÐµÐ½Ð½Ñ Ð±Ð°ÐºÐµÑ‚Ñ–Ð² (Ð· Ð½Ð¾Ð²Ð¸Ð¼ Ñ†Ð¸ÐºÐ»Ð¾Ð¼, ÑÐºÐ¸Ð¹ Ð¿ÐµÑ€ÐµÐ´Ð°Ñ” data_index):\n",
        "for data_index, vector in enumerate(X_tfidf):\n",
        "    # Ð—Ð¼Ñ–Ð½ÐµÐ½Ð¾: Ñ‚ÐµÐ¿ÐµÑ€ add_vector Ð¿Ñ€Ð¸Ð¹Ð¼Ð°Ñ” Ñ–Ð½Ð´ÐµÐºÑ\n",
        "    lsh_model.add_vector(vector, data_index)\n",
        "\n",
        "query_vector_lsh = X_tfidf[0]\n",
        "similar_items_indices = lsh_model.query(query_vector_lsh)\n",
        "\n",
        "print(f\"## âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ LSH Ð¿Ñ–ÑÐ»Ñ Ð²Ð¸Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ñ TypeError\")\n",
        "print(f\"ÐšÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ Ñ…ÐµÑˆ-Ñ„ÑƒÐ½ÐºÑ†Ñ–Ð¹: 15\")\n",
        "print(f\"Ð—Ð°Ð³Ð°Ð»ÑŒÐ½Ð° ÐºÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¸Ñ… 'ÑÑ…Ð¾Ð¶Ð¸Ñ…' (ÑƒÐ½Ñ–ÐºÐ°Ð»ÑŒÐ½Ð¸Ñ…) Ñ–Ð½Ð´ÐµÐºÑÑ–Ð²: {len(similar_items_indices)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M2o7U05gyuH",
        "outputId": "a7754ca3-5e75-48f5-b5ad-d0b2e25db68a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## âœ… Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ LSH Ð¿Ñ–ÑÐ»Ñ Ð²Ð¸Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð½Ñ TypeError\n",
            "ÐšÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ Ñ…ÐµÑˆ-Ñ„ÑƒÐ½ÐºÑ†Ñ–Ð¹: 15\n",
            "Ð—Ð°Ð³Ð°Ð»ÑŒÐ½Ð° ÐºÑ–Ð»ÑŒÐºÑ–ÑÑ‚ÑŒ Ð·Ð½Ð°Ð¹Ð´ÐµÐ½Ð¸Ñ… 'ÑÑ…Ð¾Ð¶Ð¸Ñ…' (ÑƒÐ½Ñ–ÐºÐ°Ð»ÑŒÐ½Ð¸Ñ…) Ñ–Ð½Ð´ÐµÐºÑÑ–Ð²: 5571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "test_queries = [\n",
        "    \"You can win a free car!! Click on the link\"\n",
        "]\n",
        "X_test_queries = tfidf.transform(test_queries).toarray()\n",
        "query_vector = X_test_queries[0] # Ð’ÐµÐºÑ‚Ð¾Ñ€ Q1\n",
        "similar_indices = lsh_model.query(query_vector)\n",
        "\n",
        "if not similar_indices:\n",
        "    print(\"LSH Ð½Ðµ Ð·Ð½Ð°Ð¹ÑˆÐ¾Ð² Ð¶Ð¾Ð´Ð½Ð¸Ñ… ÑÑ…Ð¾Ð¶Ð¸Ñ… Ñ–Ð½Ð´ÐµÐºÑÑ–Ð² Ñƒ ÑÐ²Ð¾Ñ—Ñ… Ð±Ð°ÐºÐµÑ‚Ð°Ñ….\")\n",
        "else:\n",
        "    # 1. ÐžÑ‚Ñ€Ð¸Ð¼Ð°Ð½Ð½Ñ Ð²ÐµÐºÑ‚Ð¾Ñ€Ñ–Ð², ÑÐºÑ– LSH Ð²Ð²Ð°Ð¶Ð°Ñ” ÑÑ…Ð¾Ð¶Ð¸Ð¼Ð¸\n",
        "    X_similar = X_tfidf[similar_indices]\n",
        "\n",
        "    # 2. ÐžÐ±Ñ‡Ð¸ÑÐ»ÐµÐ½Ð½Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾Ñ— ÐºÐ¾ÑÐ¸Ð½ÑƒÑÐ½Ð¾Ñ— ÑÑ…Ð¾Ð¶Ð¾ÑÑ‚Ñ–\n",
        "    similarities = cosine_similarity(query_vector.reshape(1, -1), X_similar).flatten()\n",
        "\n",
        "    # 3. Ð¡Ñ‚Ð²Ð¾Ñ€ÐµÐ½Ð½Ñ DataFrame Ð´Ð»Ñ ÑÐ¾Ñ€Ñ‚ÑƒÐ²Ð°Ð½Ð½Ñ\n",
        "    results = pd.DataFrame({\n",
        "        'original_index': similar_indices,\n",
        "        'similarity': similarities\n",
        "    })\n",
        "\n",
        "    # 4. Ð’Ð¸Ð±Ñ–Ñ€ Ñ‚Ð¾Ð¿-3 Ð½Ð°Ð¹ÑÑ…Ð¾Ð¶Ñ–ÑˆÐ¸Ñ…\n",
        "    top_results = results.sort_values(by='similarity', ascending=False).head(3)\n",
        "\n",
        "    print(f\"\\n## âœ… LSH: Ð¢Ð¾Ð¿-3 ÑÑ…Ð¾Ð¶Ð¸Ñ… Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½ÑŒ Ð´Ð»Ñ '{test_queries[0][:30]}...'\")\n",
        "\n",
        "    for i, row in top_results.iterrows():\n",
        "        original_idx = int(row['original_index'])\n",
        "        similarity_score = row['similarity']\n",
        "\n",
        "        # Ð’Ð¸Ð²ÐµÐ´ÐµÐ½Ð½Ñ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ–Ð² Ð· Ð¾Ñ€Ð¸Ð³Ñ–Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð´Ð°Ñ‚Ð°ÑÐµÑ‚Ñƒ\n",
        "        label = df.iloc[original_idx]['label']\n",
        "        message = df.iloc[original_idx]['message']\n",
        "\n",
        "        print(f\"\\n{i+1}. Ð¡Ñ…Ð¾Ð¶Ñ–ÑÑ‚ÑŒ: {similarity_score:.4f} (Ð†Ð½Ð´ÐµÐºÑ: {original_idx}, ÐšÐ»Ð°Ñ: {label})\")\n",
        "        print(f\"Ð¢ÐµÐºÑÑ‚: '{message}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Glfg494YlFbL",
        "outputId": "8b376b47-88ff-448f-925b-ba3c3bb8cf55"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## âœ… LSH: Ð¢Ð¾Ð¿-3 ÑÑ…Ð¾Ð¶Ð¸Ñ… Ð¿Ð¾Ð²Ñ–Ð´Ð¾Ð¼Ð»ÐµÐ½ÑŒ Ð´Ð»Ñ 'You can win a free car!! Click...'\n",
            "\n",
            "38. Ð¡Ñ…Ð¾Ð¶Ñ–ÑÑ‚ÑŒ: 0.6537 (Ð†Ð½Ð´ÐµÐºÑ: 37, ÐšÐ»Ð°Ñ: ham)\n",
            "Ð¢ÐµÐºÑÑ‚: 'I see the letter B on my car'\n",
            "\n",
            "2919. Ð¡Ñ…Ð¾Ð¶Ñ–ÑÑ‚ÑŒ: 0.5452 (Ð†Ð½Ð´ÐµÐºÑ: 2918, ÐšÐ»Ð°Ñ: ham)\n",
            "Ð¢ÐµÐºÑÑ‚: 'Is xy in ur car when u picking me up?'\n",
            "\n",
            "5476. Ð¡Ñ…Ð¾Ð¶Ñ–ÑÑ‚ÑŒ: 0.5211 (Ð†Ð½Ð´ÐµÐºÑ: 5475, ÐšÐ»Ð°Ñ: ham)\n",
            "Ð¢ÐµÐºÑÑ‚: 'Dhoni have luck to win some big title.so we will win:)'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ð›Ð¾Ð³Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð° Ñ€ÐµÐ³Ñ€ÐµÑÑ–Ñ"
      ],
      "metadata": {
        "id": "UeujSh2hnK_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ð Ð¾Ð·Ð´Ñ–Ð»ÐµÐ½Ð½Ñ Ð´Ð°Ð½Ð¸Ñ… Ð½Ð° Ñ‚Ñ€ÐµÐ½ÑƒÐ²Ð°Ð»ÑŒÐ½Ð¸Ð¹ Ñ‚Ð° Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¸Ð¹ Ð½Ð°Ð±Ð¾Ñ€Ð¸\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ð†Ð½Ñ–Ñ†Ñ–Ð°Ð»Ñ–Ð·Ð°Ñ†Ñ–Ñ Ñ‚Ð° Ð½Ð°Ð²Ñ‡Ð°Ð½Ð½Ñ Ð¼Ð¾Ð´ÐµÐ»Ñ– Ð»Ð¾Ð³Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð¾Ñ— Ñ€ÐµÐ³Ñ€ÐµÑÑ–Ñ—\n",
        "log_reg = LogisticRegression(solver='liblinear', random_state=42)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð·ÑƒÐ²Ð°Ð½Ð½Ñ Ð½Ð° Ñ‚ÐµÑÑ‚Ð¾Ð²Ð¾Ð¼Ñƒ Ð½Ð°Ð±Ð¾Ñ€Ñ–\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# ÐžÑ†Ñ–Ð½ÐºÐ° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ–Ð²\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
        "\n",
        "print(f\"\\n## ðŸ“ˆ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸ ÐºÐ»Ð°ÑÐ¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–Ñ— (Ð›Ð¾Ð³Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð° Ñ€ÐµÐ³Ñ€ÐµÑÑ–Ñ)\")\n",
        "print(f\"Ð¢Ð¾Ñ‡Ð½Ñ–ÑÑ‚ÑŒ (Accuracy): {accuracy:.4f}\")\n",
        "print(\"\\nÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ñ Ð¿Ð»ÑƒÑ‚Ð°Ð½Ð¸Ð½Ð¸ (Confusion Matrix):\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nÐ—Ð²Ñ–Ñ‚ ÐºÐ»Ð°ÑÐ¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–Ñ— (Classification Report):\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9phhGC6g2ze",
        "outputId": "382e1990-2d97-4ff2-f346-5f3097ffe22c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## ðŸ“ˆ Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¸ ÐºÐ»Ð°ÑÐ¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–Ñ— (Ð›Ð¾Ð³Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð° Ñ€ÐµÐ³Ñ€ÐµÑÑ–Ñ)\n",
            "Ð¢Ð¾Ñ‡Ð½Ñ–ÑÑ‚ÑŒ (Accuracy): 0.9695\n",
            "\n",
            "ÐœÐ°Ñ‚Ñ€Ð¸Ñ†Ñ Ð¿Ð»ÑƒÑ‚Ð°Ð½Ð¸Ð½Ð¸ (Confusion Matrix):\n",
            "[[961   4]\n",
            " [ 30 120]]\n",
            "\n",
            "Ð—Ð²Ñ–Ñ‚ ÐºÐ»Ð°ÑÐ¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–Ñ— (Classification Report):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.97      1.00      0.98       965\n",
            "        spam       0.97      0.80      0.88       150\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.97      0.90      0.93      1115\n",
            "weighted avg       0.97      0.97      0.97      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ð’Ð¸ÐºÐ¾Ñ€Ð¸ÑÑ‚Ð°Ð½Ð½Ñ Ð½Ð°Ð²Ñ‡ÐµÐ½Ð¾Ñ— Ð¼Ð¾Ð´ÐµÐ»Ñ– log_reg\n",
        "try:\n",
        "    prediction = log_reg.predict(X_test_queries)[0]\n",
        "    probability = log_reg.predict_proba(X_test_queries)[0, 1]\n",
        "    predicted_label = 'SPAM' if prediction == 1 else 'HAM'\n",
        "\n",
        "    print(f\"\\n## ðŸŽ¯ ÐšÐ»Ð°ÑÐ¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–Ñ (Ð›Ð¾Ð³Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð° Ñ€ÐµÐ³Ñ€ÐµÑÑ–Ñ) Ð´Ð»Ñ Ð·Ð°Ð¿Ð¸Ñ‚Ñƒ Q4\")\n",
        "    print(f\"Ð¢ÐµÐºÑÑ‚: '{test_queries[0]}'\")\n",
        "    print(f\"ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð·: **{predicted_label}** (Ð™Ð¼Ð¾Ð²Ñ–Ñ€Ð½Ñ–ÑÑ‚ÑŒ SPAM: {probability:.4f})\")\n",
        "except NameError:\n",
        "    print(\"ÐŸÐ¾Ð¼Ð¸Ð»ÐºÐ°: Ð—Ð¼Ñ–Ð½Ð½Ð° 'log_reg' (Ð½Ð°Ð²Ñ‡ÐµÐ½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ) Ð½Ðµ Ð²Ð¸Ð·Ð½Ð°Ñ‡ÐµÐ½Ð°.\")"
      ],
      "metadata": {
        "id": "w3OXEdU5musF",
        "outputId": "64b6ee15-0871-40d0-bcb5-cee2625b4c17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## ðŸŽ¯ ÐšÐ»Ð°ÑÐ¸Ñ„Ñ–ÐºÐ°Ñ†Ñ–Ñ (Ð›Ð¾Ð³Ñ–ÑÑ‚Ð¸Ñ‡Ð½Ð° Ñ€ÐµÐ³Ñ€ÐµÑÑ–Ñ) Ð´Ð»Ñ Ð·Ð°Ð¿Ð¸Ñ‚Ñƒ Q4\n",
            "Ð¢ÐµÐºÑÑ‚: 'You can win a free car!! Click on the link'\n",
            "ÐŸÑ€Ð¾Ð³Ð½Ð¾Ð·: **HAM** (Ð™Ð¼Ð¾Ð²Ñ–Ñ€Ð½Ñ–ÑÑ‚ÑŒ SPAM: 0.4366)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ÐÐš_7",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}