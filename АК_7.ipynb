{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olgasherbiena/KPI/blob/main/%D0%90%D0%9A_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABND7PIbPhQW",
        "outputId": "a8ff298f-9d20-4046-913a-6c885d3f3a3b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import kagglehub\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "csv_path  = kagglehub.dataset_download(\"shantanudhakadd/email-spam-detection-dataset-classification\")\n",
        "csv_file = os.path.join(csv_path, 'spam.csv')\n",
        "df = pd.read_csv(csv_file, encoding='latin1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m74dbY160h4m",
        "outputId": "e860974f-5159-4656-8c93-b1160ce6bbaf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'email-spam-detection-dataset-classification' dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö"
      ],
      "metadata": {
        "id": "GiiQuiNDhu72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['v1', 'v2']] # v1: 'label', v2: 'text'\n",
        "df.columns = ['label', 'message']\n",
        "df = df.iloc[:,:2]\n",
        "df.columns = ['label', 'message']\n",
        "le = LabelEncoder()\n",
        "df['label_encoded'] = le.fit_transform(df['label'])\n",
        "\n",
        "# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é TF-IDF\n",
        "# TF-IDF –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î —Ç–µ–∫—Å—Ç –Ω–∞ —á–∏—Å–ª–æ–≤—ñ –≤–µ–∫—Ç–æ—Ä–∏, –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –¥–ª—è LSH —Ç–∞ Logistic Regression\n",
        "tfidf = TfidfVectorizer(stop_words='english', max_features=2000) # –û–±–º–µ–∂—É—î–º–æ —Ä–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å –¥–ª—è –ø—Ä–∏–∫–ª–∞–¥—É\n",
        "X_tfidf = tfidf.fit_transform(df['message']).toarray()\n",
        "y = df['label_encoded'].values"
      ],
      "metadata": {
        "id": "tBJBOI7Igw-5"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–†–µ–∞–ª—ñ–∑–∞—Ü—ñ—è"
      ],
      "metadata": {
        "id": "YOzGlZBPhzyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class LSH:\n",
        "    def __init__(self, num_buckets, num_hash_functions, input_dim):\n",
        "        self.num_buckets = num_buckets\n",
        "        self.num_hash_functions = num_hash_functions\n",
        "        self.input_dim = input_dim\n",
        "        self.hash_functions = [self._generate_hash_function() for _ in range(num_hash_functions)]\n",
        "        self.buckets = {i: [] for i in range(num_buckets)}\n",
        "\n",
        "    def _generate_hash_function(self):\n",
        "        return np.random.randn(self.input_dim)\n",
        "\n",
        "    def hash(self, vector):\n",
        "        hashes = []\n",
        "        for hash_function in self.hash_functions:\n",
        "            hash_value = np.dot(vector, hash_function)\n",
        "            hash_value = hash_value / np.linalg.norm(hash_function)\n",
        "            # –ü—Ä–∏–º—ñ—Ç–∫–∞: –¥–ª—è LSH (–Ω–∞ –æ—Å–Ω–æ–≤—ñ SimHash) —á–∞—Å—Ç–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å np.sign,\n",
        "            # –∞–ª–µ –º–∏ –∑–∞–ª–∏—à–∞—î–º–æ –≤–∞—à—É –ª–æ–≥—ñ–∫—É –∑ np.astype(int) –¥–ª—è —Ü—ñ–ª–∏—Ö —Ö–µ—à—ñ–≤.\n",
        "            hash_value = hash_value.astype(int)\n",
        "            hashes.append(hash_value)\n",
        "        return hashes\n",
        "\n",
        "    def add_vector(self, vector, data_index):\n",
        "        hashes = self.hash(vector)\n",
        "        for hash_value in hashes:\n",
        "            self.buckets[hash_value % self.num_buckets].append((data_index, vector))\n",
        "\n",
        "    def query(self, query_vector):\n",
        "        query_hashes = self.hash(query_vector)\n",
        "        similar_indices = set()\n",
        "\n",
        "        for hash_value in query_hashes:\n",
        "            bucket = self.buckets[hash_value % self.num_buckets]\n",
        "\n",
        "            for index, vector in bucket:\n",
        "\n",
        "                if np.array_equal(vector, query_vector):\n",
        "                    continue\n",
        "                similar_indices.add(index)\n",
        "        return list(similar_indices)\n",
        "\n",
        "# --- 2. –Ü–ù–Ü–¶–Ü–ê–õ–Ü–ó–ê–¶–Ü–Ø –¢–ê –í–ò–ö–û–ù–ê–ù–ù–Ø ---\n",
        "\n",
        "input_dim = X_tfidf.shape[1]\n",
        "lsh_model = LSH(num_buckets=100, num_hash_functions=15, input_dim=input_dim)\n",
        "for data_index, vector in enumerate(X_tfidf):\n",
        "    lsh_model.add_vector(vector, data_index)\n",
        "\n",
        "query_vector_lsh = X_tfidf[0]\n",
        "similar_items_indices = lsh_model.query(query_vector_lsh)\n",
        "\n",
        "print(f\"–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ö–µ—à-—Ñ—É–Ω–∫—Ü—ñ–π: 15\")\n",
        "print(f\"–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–Ω–∞–π–¥–µ–Ω–∏—Ö '—Å—Ö–æ–∂–∏—Ö' (—É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö) —ñ–Ω–¥–µ–∫—Å—ñ–≤: {len(similar_items_indices)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M2o7U05gyuH",
        "outputId": "37b809ae-bc37-4538-cdfd-d68d1823672e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## ‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç LSH –ø—ñ—Å–ª—è –≤–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è TypeError\n",
            "–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ö–µ—à-—Ñ—É–Ω–∫—Ü—ñ–π: 15\n",
            "–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–Ω–∞–π–¥–µ–Ω–∏—Ö '—Å—Ö–æ–∂–∏—Ö' (—É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö) —ñ–Ω–¥–µ–∫—Å—ñ–≤: 5571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "test_queries = [\n",
        "    \"You can win a free car!! Click on the link\"\n",
        "]\n",
        "X_test_queries = tfidf.transform(test_queries).toarray()\n",
        "query_vector = X_test_queries[0] # –í–µ–∫—Ç–æ—Ä Q1\n",
        "similar_indices = lsh_model.query(query_vector)\n",
        "\n",
        "if not similar_indices:\n",
        "    print(\"LSH –Ω–µ –∑–Ω–∞–π—à–æ–≤ –∂–æ–¥–Ω–∏—Ö —Å—Ö–æ–∂–∏—Ö —ñ–Ω–¥–µ–∫—Å—ñ–≤ —É —Å–≤–æ—ó—Ö –±–∞–∫–µ—Ç–∞—Ö.\")\n",
        "else:\n",
        "    # 1. –û—Ç—Ä–∏–º–∞–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä—ñ–≤, —è–∫—ñ LSH –≤–≤–∞–∂–∞—î —Å—Ö–æ–∂–∏–º–∏\n",
        "    X_similar = X_tfidf[similar_indices]\n",
        "\n",
        "    # 2. –û–±—á–∏—Å–ª–µ–Ω–Ω—è —Ç–æ—á–Ω–æ—ó –∫–æ—Å–∏–Ω—É—Å–Ω–æ—ó —Å—Ö–æ–∂–æ—Å—Ç—ñ\n",
        "    similarities = cosine_similarity(query_vector.reshape(1, -1), X_similar).flatten()\n",
        "\n",
        "    # 3. –°—Ç–≤–æ—Ä–µ–Ω–Ω—è DataFrame –¥–ª—è —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è\n",
        "    results = pd.DataFrame({\n",
        "        'original_index': similar_indices,\n",
        "        'similarity': similarities\n",
        "    })\n",
        "\n",
        "    # 4. –í–∏–±—ñ—Ä —Ç–æ–ø-3 –Ω–∞–π—Å—Ö–æ–∂—ñ—à–∏—Ö\n",
        "    top_results = results.sort_values(by='similarity', ascending=False).head(3)\n",
        "\n",
        "    print(f\"\\n## ‚úÖ LSH: –¢–æ–ø-3 —Å—Ö–æ–∂–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –¥–ª—è '{test_queries[0][:30]}...'\")\n",
        "\n",
        "    for i, row in top_results.iterrows():\n",
        "        original_idx = int(row['original_index'])\n",
        "        similarity_score = row['similarity']\n",
        "\n",
        "        # –í–∏–≤–µ–¥–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç—É\n",
        "        label = df.iloc[original_idx]['label']\n",
        "        message = df.iloc[original_idx]['message']\n",
        "\n",
        "        print(f\"\\n{i+1}. –°—Ö–æ–∂—ñ—Å—Ç—å: {similarity_score:.4f} (–Ü–Ω–¥–µ–∫—Å: {original_idx}, –ö–ª–∞—Å: {label})\")\n",
        "        print(f\"–¢–µ–∫—Å—Ç: '{message}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Glfg494YlFbL",
        "outputId": "35af78b3-462f-4909-fefc-39d68a3ae5f5"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## ‚úÖ LSH: –¢–æ–ø-3 —Å—Ö–æ–∂–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å –¥–ª—è 'You can win a free car!! Click...'\n",
            "\n",
            "16. –°—Ö–æ–∂—ñ—Å—Ç—å: 0.4614 (–Ü–Ω–¥–µ–∫—Å: 15, –ö–ª–∞—Å: spam)\n",
            "–¢–µ–∫—Å—Ç: 'XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL'\n",
            "\n",
            "5254. –°—Ö–æ–∂—ñ—Å—Ç—å: 0.3312 (–Ü–Ω–¥–µ–∫—Å: 5253, –ö–ª–∞—Å: ham)\n",
            "–¢–µ–∫—Å—Ç: 'Please tell me not all of my car keys are in your purse'\n",
            "\n",
            "2805. –°—Ö–æ–∂—ñ—Å—Ç—å: 0.3302 (–Ü–Ω–¥–µ–∫—Å: 2804, –ö–ª–∞—Å: ham)\n",
            "–¢–µ–∫—Å—Ç: 'I think it's all still in my car'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è"
      ],
      "metadata": {
        "id": "UeujSh2hnK_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö –Ω–∞ —Ç—Ä–µ–Ω—É–≤–∞–ª—å–Ω–∏–π —Ç–∞ —Ç–µ—Å—Ç–æ–≤–∏–π –Ω–∞–±–æ—Ä–∏\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–∞ –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –ª–æ–≥—ñ—Å—Ç–∏—á–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó\n",
        "log_reg = LogisticRegression(solver='liblinear', random_state=42)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –Ω–∞–±–æ—Ä—ñ\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# –û—Ü—ñ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
        "\n",
        "print(f\"\\n## üìà –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó (–õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è)\")\n",
        "print(f\"–¢–æ—á–Ω—ñ—Å—Ç—å (Accuracy): {accuracy:.4f}\")\n",
        "print(\"\\n–ú–∞—Ç—Ä–∏—Ü—è –ø–ª—É—Ç–∞–Ω–∏–Ω–∏ (Confusion Matrix):\")\n",
        "print(conf_matrix)\n",
        "print(\"\\n–ó–≤—ñ—Ç –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó (Classification Report):\")\n",
        "print(class_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9phhGC6g2ze",
        "outputId": "7d45730d-6ec4-4078-973f-a91494911594"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## üìà –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó (–õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è)\n",
            "–¢–æ—á–Ω—ñ—Å—Ç—å (Accuracy): 0.9596\n",
            "\n",
            "–ú–∞—Ç—Ä–∏—Ü—è –ø–ª—É—Ç–∞–Ω–∏–Ω–∏ (Confusion Matrix):\n",
            "[[962   3]\n",
            " [ 42 108]]\n",
            "\n",
            "–ó–≤—ñ—Ç –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó (Classification Report):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.96      1.00      0.98       965\n",
            "        spam       0.97      0.72      0.83       150\n",
            "\n",
            "    accuracy                           0.96      1115\n",
            "   macro avg       0.97      0.86      0.90      1115\n",
            "weighted avg       0.96      0.96      0.96      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –Ω–∞–≤—á–µ–Ω–æ—ó –º–æ–¥–µ–ª—ñ log_reg\n",
        "try:\n",
        "    prediction = log_reg.predict(X_test_queries)[0]\n",
        "    probability = log_reg.predict_proba(X_test_queries)[0, 1]\n",
        "    predicted_label = 'SPAM' if prediction == 1 else 'HAM'\n",
        "\n",
        "    print(f\"\\n## üéØ –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è (–õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è) –¥–ª—è –∑–∞–ø–∏—Ç—É Q4\")\n",
        "    print(f\"–¢–µ–∫—Å—Ç: '{test_queries[0]}'\")\n",
        "    print(f\"–ü—Ä–æ–≥–Ω–æ–∑: **{predicted_label}** (–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å SPAM: {probability:.4f})\")\n",
        "except NameError:\n",
        "    print(\"–ü–æ–º–∏–ª–∫–∞: –ó–º—ñ–Ω–Ω–∞ 'log_reg' (–Ω–∞–≤—á–µ–Ω–∞ –º–æ–¥–µ–ª—å) –Ω–µ –≤–∏–∑–Ω–∞—á–µ–Ω–∞.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3OXEdU5musF",
        "outputId": "b7a48bcb-1132-4f44-8746-b5912c0c7530"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "## üéØ –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è (–õ–æ–≥—ñ—Å—Ç–∏—á–Ω–∞ —Ä–µ–≥—Ä–µ—Å—ñ—è) –¥–ª—è –∑–∞–ø–∏—Ç—É Q4\n",
            "–¢–µ–∫—Å—Ç: 'You can win a free car!! Click on the link'\n",
            "–ü—Ä–æ–≥–Ω–æ–∑: **HAM** (–ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å SPAM: 0.4524)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "–ê–ö_7",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}